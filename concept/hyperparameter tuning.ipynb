{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752d92a0",
   "metadata": {},
   "source": [
    ">Hyperparameters being tuned\n",
    "\n",
    "\n",
    ">Number of hidden layers\n",
    "\n",
    ">Number of neurons (units) per layer\n",
    "\n",
    ">Activation function for each layer\n",
    "\n",
    ">Dropout rate per layer\n",
    "\n",
    ">Regularization type (none, l1, l2, or l1_l2)\n",
    "\n",
    ">Regularization strength (value)\n",
    "\n",
    ">Optimizer type (adam, rmsprop, or sgd)\n",
    "\n",
    ">Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca931c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ef6e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 02s]\n",
      "val_accuracy: 0.46875\n",
      "\n",
      "Best val_accuracy So Far: 0.981249988079071\n",
      "Total elapsed time: 00h 00m 34s\n",
      "\n",
      "✅ Best Hyperparameters Found:\n",
      "reg_type: none\n",
      "reg_value: 0.0001\n",
      "num_layers: 4\n",
      "units_0: 16\n",
      "activation_0: relu\n",
      "dropout_0: 0.5\n",
      "units_1: 96\n",
      "activation_1: tanh\n",
      "dropout_1: 0.2\n",
      "optimizer: adam\n",
      "lr: 0.003818588357106265\n",
      "units_2: 32\n",
      "activation_2: elu\n",
      "dropout_2: 0.5\n",
      "units_3: 16\n",
      "activation_3: relu\n",
      "dropout_3: 0.1\n",
      "\n",
      "🎯 Test Accuracy: 0.9700\n",
      "🧾 Test Loss: 0.1083\n",
      "\n",
      "🚀 Retraining best model on full training data...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/Desktop/deep-learning/deep_learning/lib/python3.13/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9112 - loss: 0.2311 - val_accuracy: 0.9750 - val_loss: 0.0935 - learning_rate: 0.0038\n",
      "Epoch 2/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9125 - loss: 0.2185 - val_accuracy: 0.9750 - val_loss: 0.0896 - learning_rate: 0.0038\n",
      "Epoch 3/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9162 - loss: 0.2309 - val_accuracy: 0.9650 - val_loss: 0.0920 - learning_rate: 0.0038\n",
      "Epoch 4/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9050 - loss: 0.2370 - val_accuracy: 0.9750 - val_loss: 0.0757 - learning_rate: 0.0038\n",
      "Epoch 5/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8925 - loss: 0.2392 - val_accuracy: 0.9750 - val_loss: 0.0752 - learning_rate: 0.0038\n",
      "Epoch 6/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.2201 - val_accuracy: 0.9700 - val_loss: 0.0685 - learning_rate: 0.0038\n",
      "Epoch 7/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.2306 - val_accuracy: 0.9700 - val_loss: 0.0675 - learning_rate: 0.0038\n",
      "Epoch 8/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9300 - loss: 0.2079 - val_accuracy: 0.9650 - val_loss: 0.0826 - learning_rate: 0.0038\n",
      "Epoch 9/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9212 - loss: 0.1942 - val_accuracy: 0.9800 - val_loss: 0.0743 - learning_rate: 0.0038\n",
      "Epoch 10/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.2278 - val_accuracy: 0.9750 - val_loss: 0.0774 - learning_rate: 0.0038\n",
      "Epoch 11/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9137 - loss: 0.2109 - val_accuracy: 0.9550 - val_loss: 0.1019 - learning_rate: 0.0038\n",
      "Epoch 12/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9237 - loss: 0.1994 - val_accuracy: 0.9750 - val_loss: 0.0675 - learning_rate: 0.0038\n",
      "Epoch 13/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9212 - loss: 0.1833 - val_accuracy: 0.9750 - val_loss: 0.0657 - learning_rate: 0.0019\n",
      "Epoch 14/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9325 - loss: 0.1902 - val_accuracy: 0.9550 - val_loss: 0.0929 - learning_rate: 0.0019\n",
      "Epoch 15/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9237 - loss: 0.2151 - val_accuracy: 0.9650 - val_loss: 0.0864 - learning_rate: 0.0019\n",
      "Epoch 16/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9237 - loss: 0.2001 - val_accuracy: 0.9700 - val_loss: 0.0799 - learning_rate: 0.0019\n",
      "Epoch 17/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9250 - loss: 0.2020 - val_accuracy: 0.9750 - val_loss: 0.0767 - learning_rate: 0.0019\n",
      "Epoch 18/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9312 - loss: 0.1838 - val_accuracy: 0.9600 - val_loss: 0.0774 - learning_rate: 0.0019\n",
      "Epoch 19/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9225 - loss: 0.2024 - val_accuracy: 0.9650 - val_loss: 0.0801 - learning_rate: 9.5465e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9337 - loss: 0.1933 - val_accuracy: 0.9650 - val_loss: 0.0719 - learning_rate: 9.5465e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9475 - loss: 0.1678 - val_accuracy: 0.9700 - val_loss: 0.0724 - learning_rate: 9.5465e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9275 - loss: 0.1734 - val_accuracy: 0.9600 - val_loss: 0.0812 - learning_rate: 9.5465e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9225 - loss: 0.1956 - val_accuracy: 0.9750 - val_loss: 0.0694 - learning_rate: 9.5465e-04\n",
      "\n",
      "✅ Final Test Accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# === 1. Build Model Function for Keras Tuner ===\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Regularization type and value\n",
    "    reg_type = hp.Choice(\"reg_type\", [\"none\", \"l1\", \"l2\", \"l1_l2\"])\n",
    "    reg_value = hp.Choice(\"reg_value\", [1e-5, 1e-4, 1e-3])\n",
    "\n",
    "    if reg_type == \"l1\":\n",
    "        reg = regularizers.l1(reg_value)\n",
    "    elif reg_type == \"l2\":\n",
    "        reg = regularizers.l2(reg_value)\n",
    "    elif reg_type == \"l1_l2\":\n",
    "        reg = regularizers.l1_l2(l1=reg_value, l2=reg_value)\n",
    "    else:\n",
    "        reg = None\n",
    "\n",
    "    # Hidden layers\n",
    "    num_layers = hp.Int(\"num_layers\", min_value=2, max_value=5)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", min_value=16, max_value=128, step=16)\n",
    "        activation = hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"elu\"])\n",
    "\n",
    "        if i == 0:\n",
    "            model.add(Dense(units, activation=activation, input_dim=2, kernel_regularizer=reg))\n",
    "        else:\n",
    "            model.add(Dense(units, activation=activation, kernel_regularizer=reg))\n",
    "\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float(f\"dropout_{i}\", 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Optimizer and learning rate\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == \"rmsprop\":\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# === 2. Create Tuner ===\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,                  # Try up to 30 combinations\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_results\",\n",
    "    project_name=\"classification_tuning\"\n",
    ")\n",
    "\n",
    "\n",
    "# === 3. Define Callbacks ===\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# === 4. Run Hyperparameter Search ===\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# === 5. Get the Best Model ===\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n✅ Best Hyperparameters Found:\")\n",
    "for key, value in best_hps.values.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# === 6. Evaluate on Test Set ===\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n🎯 Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"🧾 Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# === 7. (Optional) Retrain on Full Training Data ===\n",
    "print(\"\\n🚀 Retraining best model on full training data...\")\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === 8. Final Evaluation ===\n",
    "final_loss, final_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n✅ Final Test Accuracy: {final_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbe950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
