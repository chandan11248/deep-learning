# ğŸš€ Deep Learning Projects and Concepts

![GitHub Repo Size](https://img.shields.io/github/repo-size/chandan11248/deep-learning)
![GitHub stars](https://img.shields.io/github/stars/chandan11248/deep-learning?style=social)
![GitHub forks](https://img.shields.io/github/forks/chandan11248/deep-learning?style=social)
![Python](https://img.shields.io/badge/python-3.11-blue)

Welcome to my **collection of deep learning projects**! ğŸŒŸ  
This repository showcases various neural network architectures and their applications in image processing, sequence prediction, multi-output modeling, and more.

---

## ğŸ“‚ Repository Structure

The repository is organized into the following directories:

- **ANN/** : Implementations of Artificial Neural Networks.  
- **CNN/** : Convolutional Neural Networks for image-related tasks.  
- **RNN/** : Recurrent Neural Networks for sequence data.    
- **\*.ipynb files** : Jupyter Notebooks demonstrating various models and experiments and projects

---

## ğŸŒŸ Project Highlights

### 1. ğŸ§  Artificial Neural Networks (ANN)
- Explore basic to advanced ANN architectures.  
- Tasks include classification and regression.  
- Example: Predicting house prices, digit recognition.  

### 2. ğŸ–¼ï¸ Convolutional Neural Networks (CNN)
- CNNs for image classification, object detection, and feature extraction.  
- Understand convolution, pooling, and feature extraction.  
- Example: CIFAR-10, MNIST classification.  

### 3. ğŸ” Recurrent Neural Networks (RNN)
- RNNs and variants (SimpleRNN, GRU, LSTM) for sequence prediction.  
- Applications: Time series forecasting, NLP, sentiment analysis.  


### 4. ğŸ“Š Multi-Output Models
- Models predicting more than one target simultaneously.  
- Example: Age & gender prediction from images.  



## âš¡ Transformers & Attention (NEW)

Transformers revolutionized NLP and now dominate many AI tasks.  
They replace RNNs by using **self-attention**, allowing models to learn relationships between tokens in parallel.

### ğŸ”‘ Key Concepts
- **Self-Attention**: Each token attends to all others  
- **Multi-Head Attention**: Multiple attention heads capture richer features  
- **Positional Encoding**: Adds sequence order information  
- **Encoder / Decoder Architecture**:  
  - Encoder â†’ representation tasks  
  - Decoder â†’ autoregressive generation  

Transformers handle long-range dependencies extremely well and enable massive parallelism.  
They are used in NLP, vision (ViT), speech, and multimodal models.

---

## ğŸ¤– GPT (Generative Pretrained Transformer)

GPT is a **decoder-only** transformer trained via **next-token prediction**.

### Features
- Autoregressive (left-to-right generation)  
- Causal masking  
- Excellent at text generation, conversation, reasoning, summarization  
- Scales extremely well with large datasets and model sizes  



---

## ğŸ§  BERT (Bidirectional Encoder Representations from Transformers)

BERT is an **encoder-only** transformer trained using **masked language modeling (MLM)**.

### Features
- Deep bidirectional understanding  
- Great for classification, QA, NER, embeddings  
- Learns context from both left & right of a token  
- Often fine-tuned for downstream tasks  


---

## ğŸš€ Getting Started

To run the projects locally:

1. **Clone the repository:**

git clone https://github.com/chandan11248/deep-learning.git

    â€¢	cd deep-learning

    
2. **Install dependencies:**

        â€¢	pip install -r requirements.txt

    
3. **Navigate to the desired project directory and run the corresponding Jupyter Notebook:**

            jupyter notebook project.ipynb



---

## ğŸ“š Resources
	â€¢	datacamp
	â€¢	documentation
	â€¢	youtube


Feel free to explore the concepts, contribute, or reach out if you have any questions or suggestions!